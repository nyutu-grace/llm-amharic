{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace-nyutu/Documents/llm-amharic/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer, \n",
    "    TrainerCallback, \n",
    "    default_data_collator,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(bnb_config):\n",
    "\n",
    "    model_name = \"../model/base/Llama-2-7b-hf\"\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{21960}MB'\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", \n",
    "        torch_dtype=torch.float16 \n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m bnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     11\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n\u001b[0;32m---> 13\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(bnb_config)\u001b[0m\n\u001b[1;32m      5\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m21960\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mMB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m~/Documents/llm-amharic/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3202\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3202\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3205\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3206\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/Documents/llm-amharic/venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "load_in_4bit = True\n",
    "\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "\n",
    "bnb_4bit_compute_dtype = torch.bfloat16\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"../data/summary.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files = dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 95141\n",
      "Column names are: ['text', 'summary']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '(ሲ.ኤን.ኤን.) - የ14 ዓመቷ Snigdha Nandipati ሐሙስ ምሽት የስክሪፕስ ናሽናል ሆሄሊንግ ንብን \"ጉቴፔንስ\" ብሎ በመፃፍ አሸንፏል፣ ትርጉሙም አድብቶ፣ ወጥመድ ወይም ወጥመድ ማለት ነው። የሳንዲያጎ የስምንተኛ ክፍል ተማሪ አርብ ጥዋት ለ CNN እንደተናገረው \"በአንድ ጊዜ አንድ ቃል እየወሰድኩ ነበር.\" \"እኔ ብቻ እያንዳንዱን ቃል በትክክል ማግኘት ፈልጎ ነበር, እኔ በእርግጥ ለማሸነፍ አላሰብኩም ነበር, በእርግጥ.\" ከፈረንሳይኛ የተወሰደውን የአሸናፊውን ቃል በትክክል መፃፍ አስቸጋሪ እንዳልሆነ ተናግራለች። ቃሉን ከዚህ በፊት አይታ ነበር እና አውቀዋለች አለች ። ኮንፈቲ መውደቅ እስኪጀምር ድረስ ናንዲፓቲ ድሏን በትክክል አላስመዘገበችም አለች ። \"አሸንፋለሁ ብዬ አልጠበኩም ነበር በዚህ አመት ጥሩ ጥሩ ተወዳዳሪዎች ነበሩ\" ትላለች። ባለፈው አመት የፊደል አጻጻፍ ንብ 27ኛ ደረጃን አግኝታለች። የብሔራዊ ፊደል ንብ ሥቃይ እና ደስታ። በ13ኛው ዙር የናንዲፓቲ ድል የተገኘው የ14 ዓመቷ የኦርላንዶ የመጨረሻ ተፎካካሪዋ ስቱቲ ሚሽራ “schwarmerei” በሚለው የፊደል አጻጻፍ ከተደናቀፈች በኋላ ነው፤ ይህ ማለት ከልክ ያለፈ፣ ገደብ የለሽ ጉጉት ወይም ትስስር ማለት ነው። የ12 አመቱ የኒውዮርክ ከተማ የሰባተኛ ክፍል ተማሪ የሆነው አርቪንድ ማሃንካሊ \"schwannoma\" የተባለውን የፔሪፈራል ነርቭ ሽፋን እጢ የተሳሳተ ፊደል ሲጽፍ ሶስተኛ ወጥቷል። @ScrippsBee በትዊተር በላከው መሰረት ናንዲፓቲ በቀን ስድስት ሰአት አጥንቷል። ለማየት ከህንድ በሄዱት ወንድሟ፣ ወላጆቿ (አባቷ አሰለጠኗት) እና አያቶቿ ደስ አሰኘች። ውድድሩ የተካሄደው ከዋሽንግተን ውጭ በሚገኘው በናሽናል ሃርቦር ሜሪላንድ በሚገኘው ጌይሎርድ ናሽናል ሪዞርት እና ኮንቬንሽን ሴንተር ነው። ናንዲፓቲ 30,000 ዶላር እና የተቀረጸ ዋንጫን ከ Scripps፣ $2,500 የአሜሪካ የቁጠባ ቦንድ እና የማጣቀሻ ቤተመፃህፍት ከ Merriam-Webster፣ የ $5,000 ስኮላርሺፕ ከሲግማ ፊል Epsilon የትምህርት ፋውንዴሽን እና ከ $2,600 በላይ በማጣቀሻ ስራዎች ከ ኢንሳይክሎፔዲያ ብሪታኒካ። የዘንድሮው የፊደል አጻጻፍ ንብ ታናሽ ተወዳዳሪዋን የ6 ዓመቷን ሎሪ አን ማዲሰንን ከሐይቅ ሪጅ፣ ቨርጂኒያ አይታለች። በአፕሎም \"አስደሳች\" ብላ ጻፈች፣ነገር ግን ረቡዕ ምሽት \"ኢንሉቪስ\" ስትል ጠፋች። የ14 አመቱ ኒኮላስ ሩሽሎው የላንካስተር ኦሃዮ የስምንተኛ ክፍል ተማሪ በህንድ ውስጥ ለሽቶ ማምረቻ እና ምንጣፎች ጥቅም ላይ የሚውለውን ዘይት የሚያመርት የአየር ላይ ሳር \"ቬቲቭ\" በተሳሳተ ፊደል ሲጽፍ ተወግዷል። ቃሉን ሲሰማ በአእምሮው ውስጥ ምን እያጋጠመው እንዳለ ሲጠየቅ፣ “ኧረ ጉድ ነው” አለ። በውድድሩ ያሳየው አምስተኛ እና የመጨረሻው ነበር። ከዚህ ቀደም ለፊደል አጻጻፍ ባደረገው ጊዜ ሁሉ ምን እንደሚያደርግ ሲጠየቅ፣ “አዲስ የትርፍ ጊዜ ማሳለፊያ ማግኘት አለብኝ” ብሏል።',\n",
       " 'summary': '\"በአንድ ጊዜ አንድ ቃል እየወሰድኩ ነበር,\" Snigdha Nandipati ይላል.\\nየ14 አመቱ ወጣት ባለፈው አመት የፊደል አጻጻፍ ንብ 27ኛ ደረጃ ላይ ተቀምጧል።\\nድሏ የመጨረሻው ተፎካካሪዋ በ\"ሽዋርሜሬይ\" ከተደናቀፈ ከጥቂት ጊዜያት በኋላ የመጣ ነው።'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that is a summary of a text. Write a response that appropriately gives the complete text.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['summary']}\"\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['text']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"intro\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'የአርታዒ ማስታወሻ፡ የMyGoodDeed ተባባሪ መስራች ጄይ ኤስ ዊኑክ በሴፕቴምበር 11 ላይ የአለም ንግድ ማእከል ሳውዝ ታወር ሲወድም በግሌን ጄ. , 2001. በዚህ ሳምንት ግሌን ከዩናይትድ ስቴትስ ኦፍ አሜሪካ በ 9/11 የጀግኖች የቫሎር ሜዳሊያ ተሸልሟል። ጄይ ዊኑክ ሴፕቴምበር 11 በተሻለ ሁኔታ የሚከበረው ለሌሎች አገልግሎት ቀን ነው። ኒው ዮርክ (ሲ.ኤን.) - በመጪው መስከረም 11 ላይ የተፈጸሙት ጥቃቶች ስምንተኛ አመት በሚሊዮኖች ለሚቆጠሩ አሜሪካውያን አሳማኝ ጥያቄ አስነስቷል-ይህንን ልዩ የሆነ አሳዛኝ ቀን በሀገራችን ታሪክ እንዴት በተሻለ ሁኔታ ማክበር አለብን? በእርግጠኝነት, የበዓል ቀን መሆን የለበትም. ይህ ከስራ ለቀናት እረፍት እና የሶስት ቀን ቅዳሜና እሁድ በባርቤኪው እና በነጭ ሽያጭ ለመደሰት ጊዜ አይደለም። አይደለም መስከረም 11 የማሰላሰል ቀን ነውና ታሪካዊ እና ስሜታዊ ትርጉሙ ከጊዜ ወደ ጊዜ እየቀነሰ ወይም በምንም መልኩ መቀነስ የለበትም። በተማሩት ጠቃሚ ትምህርቶች ላይ የሚያተኩርበት ቀን ነው። የ9/11 ቤተሰብ አባል ነኝ። ደፋር ወንድሜ ግሌን ጄ ዊኑክ ከአለም ንግድ ማእከል ሁለት ብሎኮች በሚገኘው ሆላንድ እና ናይት በተባለ ትልቅ የህግ ድርጅት አጋር ነበር። ለ20 ዓመታት ያህል ግሌን የበጎ ፈቃደኝነት የእሳት አደጋ መከላከያ ሠራተኛ እና የድንገተኛ ሕክምና ቴክኒሻን ነበር። ማማዎቹ በተመታ ጊዜ ግሌን የማዳኛ መሳሪያዎችን ተበድሮ ከደህንነት ወደ ደቡብ ታወር በመሮጥ በነፍስ አድን ጥረት ላይ ለመሳተፍ ሄደ። ደቡብ ግንብ ሲፈርስ ሞተ። ከጥቃቶቹ በኋላ ብዙም ሳይቆይ፣ ከጓደኛዬ ዴቪድ ፔይን እና ከሌሎች የ9/11 ቤተሰብ አባላት እና ጓደኞች ጋር፣ MyGoodDeed የሚባል መሰረታዊ ተነሳሽነት ፈጠርኩ። ግባችን? መስከረም 11 ቀን 2001 ዓ.ም ዓመታዊ እውቅና ያለው ብሔራዊ የአገልግሎት ቀን እንዲሆን። ከጥቃቱ በኋላ መንፈሳችንን እና ሀገራችንን ለማደስ የሚረዱትን በሺህዎች የሚቆጠሩ እና ድንገተኛ፣ ሩህሩህ እና ውጤታማ በሆነ አገልግሎት ለተነሱት በሚሊዮን የሚቆጠሩ ሰዎች ግብር የምንከፍልበት ከዚህ የተሻለ መንገድ እንደማይኖር በሰፊው የተጋራነው አመለካከት ነበር። በሁሉም መሪ የ9/11 ቤተሰብ፣ በህይወት የተረፉ እና የበጎ ፍቃደኛ ድርጅቶች የተደገፈ ይህ ተነሳሽነት በዓለም ዙሪያ የሚገኙ በሚሊዮኖች የሚቆጠሩ ሰዎችን ከ50ም ግዛቶች እና ከ170 በላይ ብሄር ብሄረሰቦችና ግዛቶችን ትኩረት ስቧል። ሰዎች በእያንዳንዱ 9/11 ለማከናወን የሚፈልጉትን ማንኛውንም ዓይነት ደግነት እና አገልግሎት ይመርጣሉ። እና ትላልቅ እና ትናንሽ ድርጊቶች, በተቸገሩ ሰዎች እና ማህበረሰቦች ህይወት ላይ እውነተኛ ለውጥ እያመጡ ነው. አንዳንዶች መዋጮ ያደርጋሉ - ልብስ፣ መጽሐፍት፣ የዓይን መነፅር፣ ገንዘብ፣ ደም። አንዳንዶች ትምህርት ቤቶችን እና ፓርኮችን እና የባህር ዳርቻዎችን ለመጠገን ይረዳሉ። አንዳንዶች የእንክብካቤ ፓኬጆችን ወደ ባህር ማዶ ለወታደሮቻችን ይልካሉ ወይም በሾርባ ኩሽና ውስጥ ይሰራሉ። አንዳንዶች ዓይነ ስውራን ያነባሉ ወይም አረጋውያንን ይጎበኛሉ። ዛሬ እሑድ ጡረተኛው የኒውዮርክ ጃይንት ታላቁ ጆርጅ ማርቲን ከ9/ በኋላ ባደረጉት አገልግሎት ምክንያት በህመም ለሚሰቃዩ በሺዎች ለሚቆጠሩ የ Ground Zero የማዳን እና የማገገሚያ ሰራተኞች የጤና እንክብካቤ ገንዘብ ለማሰባሰብ ከኒውዮርክ ወደ ኒው ጀርሲ የገንዘብ ማሰባሰብያ የእግር ጉዞ እየመራ ነው። 11. ሰዎች ለመሳተፍ ምን ማድረግ እንደሚችሉ ምንም ገደብ የለም. ልብ ልንል የሚገባዉ ልክ በዚያ መስከረም ጧት በፍርሃት እንደሞቱት ሁሉ በ9/11 አገልግሎት ላይ የተሰማሩ ሰዎች የተለያዩ የፖለቲካ ምርጫዎችን፣ እድሜን፣ ዘርን፣ ሀይማኖቶችን፣ ኢኮኖሚያዊ ደረጃን እና ጂኦግራፊያዊ አቀማመጥን ያመለክታሉ። ሰሞኑን በአንዳንድ ወግ አጥባቂ ጋዜጠኞች እና ጦማሪያን ከተጠቆመው በተለየ ይህ የ9/11 አገልግሎት ክስተት አዲስ አይደለም እና በእርግጠኝነት የሴፕቴምበር 11ን ትርጉም ለማሳነስ ወይም ለፖለቲካዊ ጥቅም ሲል እንደገና ለመወሰን የታሰበ የሊበራል አጀንዳ አይደለም። በእርግጥም እያንዳንዱን 9/11 ለማክበር በአገልግሎት ላይ የመሳተፍ ሀሳብ የተጀመረው በቡሽ አስተዳደር ጊዜ ሲሆን በሁለቱም በኩል በካፒቶል ሂል ላይ በሰፊው ተደግፏል። በ2004 የዩኤስ ኮንግረስ የሴፕቴምበር 11 ቀን ብሔራዊ የአገልግሎት እና የርህራሄ ቀን መሆን አለበት በማለት የ House Congressional Resolution 473 በሙሉ ድምጽ አጽድቋል። በዚህ አመት በሚያዝያ ወር፣ ፕሬዘዳንት ኦባማ ኤድዋርድ ኤም ኬኔዲ አሜሪካን ሰርቪስ ህግን ተፈራርመዋል፣ መስከረም 11ን እንደ \"ብሄራዊ የአገልግሎት እና መታሰቢያ ቀን\" በመደበኛነት የሚያቋቁመውን የሁለት ወገን ህግ ነው። የ9/11 ማህበረሰብ ይህንን ትልቅ እርምጃ አድንቆታል። እናም ፕሬዝዳንቱ ይህን ህግ ሲፈርሙ ስመለከት ከሰርቪስ ኔሽን ጥምር አባላት ጋር እና በተለያዩ የፖለቲካ መሪዎች፣ አስተማሪዎች፣ ተማሪዎች፣ የአገልግሎት በጎ ፈቃደኞች እና ሌሎች የተከበቡ፣ እኔ ሳስበው ሁለቱንም ብርድ ብርድ ማለት እና ታላቅ ሙቀት ተሰማኝ ሌሎችን በማገልገል የሞተው ወንድሜ። በሴፕቴምበር 11 የተጎዱትን ፣ሰራተኞችን እና በጎ ፍቃደኞችን ለማክበር እንደ መንገድ አገልግሎት መጪው ትውልድ ስለ ጥቃቶቹ ብቻ ሳይሆን በአለም ላይ ያሉ ጥሩ ሰዎች ሀገራችን በከባድ ጉዳት በደረሰባት ጊዜ ምን ምላሽ እንደሰጡ ይማራሉ ። በተጨማሪም ከሴፕቴምበር 11 ቀን አመድ ለመውጣት ውጤታማ እና ትርጉም ያለው መንገድ ሆኖ ያገለግላል ይህም አገራዊ ፈተናዎቻችን እንደቀድሞው ሁሉ አሁን ናቸው። አንዳንድ ሰዎች \"በሴፕቴምበር 11 ላይ በቂ ነው. ወደ ፊት እንቀጥል\" ይላሉ. በእርግጠኝነት, ያንን ተረድቻለሁ. እያስታወስን እንኳን ወደፊት መሄድ አለብን። እናም በዚህ በዓል ላይ ዋናው ነጥብ ይህ ነው። ለተጎዱት ወይም ለጠፉት እንደ ወንድሜ እና አገራችንን ለመታደግ እና ለመጠበቅ በአገልግሎት ለተነሱት ሁሉ በጉጉት በመመልከት እና የተቸገሩትን ሰዎች እና ማህበረሰቦችን በማሻሻል ክብር እንስጥ። በታሪክ ውስጥ ይህ ቅጽበት ስለ ጥቃቶቹ ብቻ አልነበረም። ስለ ርህራሄም ነበር። ይህ ደግሞ የ9/11 ክስተቶች ትምህርት ነው፣ እና ለእኔ የዚህ ብሔራዊ የአገልግሎት እና የመታሰቢያ ቀን ይዘት ነው። በዚህ አስተያየት ውስጥ የተገለጹት አስተያየቶች የጄ ኤስ.ዊኑክ ብቻ ናቸው።',\n",
       " 'summary': 'ጄይ ዊኑክ፡ 9/11 ብሔራዊ የአገልግሎት ቀን ተብሎ ይታወቃል።\\nሥራ ለመዝለል ወይም ለመገበያየት ቀን አይደለም ይላል.\\nሰዎች ትልቅም ይሁን ትንሽ የደግነት ተግባራትን ለማድረግ እንደሚመርጡ ይናገራል።\\nዊኑክ፡ የፖለቲካ አጀንዳ የለም፣ ቡሽ እና ኦባማ ደግፈውታል።',\n",
       " 'intro': 'Below is an instruction that is a summary of a text. Write a response that appropriately gives the complete text.\\n\\n### Instruction:\\nጄይ ዊኑክ፡ 9/11 ብሔራዊ የአገልግሎት ቀን ተብሎ ይታወቃል።\\nሥራ ለመዝለል ወይም ለመገበያየት ቀን አይደለም ይላል.\\nሰዎች ትልቅም ይሁን ትንሽ የደግነት ተግባራትን ለማድረግ እንደሚመርጡ ይናገራል።\\nዊኑክ፡ የፖለቲካ አጀንዳ የለም፣ ቡሽ እና ኦባማ ደግፈውታል።\\n\\n### Response:\\nየአርታዒ ማስታወሻ፡ የMyGoodDeed ተባባሪ መስራች ጄይ ኤስ ዊኑክ በሴፕቴምበር 11 ላይ የአለም ንግድ ማእከል ሳውዝ ታወር ሲወድም በግሌን ጄ. , 2001. በዚህ ሳምንት ግሌን ከዩናይትድ ስቴትስ ኦፍ አሜሪካ በ 9/11 የጀግኖች የቫሎር ሜዳሊያ ተሸልሟል። ጄይ ዊኑክ ሴፕቴምበር 11 በተሻለ ሁኔታ የሚከበረው ለሌሎች አገልግሎት ቀን ነው። ኒው ዮርክ (ሲ.ኤን.) - በመጪው መስከረም 11 ላይ የተፈጸሙት ጥቃቶች ስምንተኛ አመት በሚሊዮኖች ለሚቆጠሩ አሜሪካውያን አሳማኝ ጥያቄ አስነስቷል-ይህንን ልዩ የሆነ አሳዛኝ ቀን በሀገራችን ታሪክ እንዴት በተሻለ ሁኔታ ማክበር አለብን? በእርግጠኝነት, የበዓል ቀን መሆን የለበትም. ይህ ከስራ ለቀናት እረፍት እና የሶስት ቀን ቅዳሜና እሁድ በባርቤኪው እና በነጭ ሽያጭ ለመደሰት ጊዜ አይደለም። አይደለም መስከረም 11 የማሰላሰል ቀን ነውና ታሪካዊ እና ስሜታዊ ትርጉሙ ከጊዜ ወደ ጊዜ እየቀነሰ ወይም በምንም መልኩ መቀነስ የለበትም። በተማሩት ጠቃሚ ትምህርቶች ላይ የሚያተኩርበት ቀን ነው። የ9/11 ቤተሰብ አባል ነኝ። ደፋር ወንድሜ ግሌን ጄ ዊኑክ ከአለም ንግድ ማእከል ሁለት ብሎኮች በሚገኘው ሆላንድ እና ናይት በተባለ ትልቅ የህግ ድርጅት አጋር ነበር። ለ20 ዓመታት ያህል ግሌን የበጎ ፈቃደኝነት የእሳት አደጋ መከላከያ ሠራተኛ እና የድንገተኛ ሕክምና ቴክኒሻን ነበር። ማማዎቹ በተመታ ጊዜ ግሌን የማዳኛ መሳሪያዎችን ተበድሮ ከደህንነት ወደ ደቡብ ታወር በመሮጥ በነፍስ አድን ጥረት ላይ ለመሳተፍ ሄደ። ደቡብ ግንብ ሲፈርስ ሞተ። ከጥቃቶቹ በኋላ ብዙም ሳይቆይ፣ ከጓደኛዬ ዴቪድ ፔይን እና ከሌሎች የ9/11 ቤተሰብ አባላት እና ጓደኞች ጋር፣ MyGoodDeed የሚባል መሰረታዊ ተነሳሽነት ፈጠርኩ። ግባችን? መስከረም 11 ቀን 2001 ዓ.ም ዓመታዊ እውቅና ያለው ብሔራዊ የአገልግሎት ቀን እንዲሆን። ከጥቃቱ በኋላ መንፈሳችንን እና ሀገራችንን ለማደስ የሚረዱትን በሺህዎች የሚቆጠሩ እና ድንገተኛ፣ ሩህሩህ እና ውጤታማ በሆነ አገልግሎት ለተነሱት በሚሊዮን የሚቆጠሩ ሰዎች ግብር የምንከፍልበት ከዚህ የተሻለ መንገድ እንደማይኖር በሰፊው የተጋራነው አመለካከት ነበር። በሁሉም መሪ የ9/11 ቤተሰብ፣ በህይወት የተረፉ እና የበጎ ፍቃደኛ ድርጅቶች የተደገፈ ይህ ተነሳሽነት በዓለም ዙሪያ የሚገኙ በሚሊዮኖች የሚቆጠሩ ሰዎችን ከ50ም ግዛቶች እና ከ170 በላይ ብሄር ብሄረሰቦችና ግዛቶችን ትኩረት ስቧል። ሰዎች በእያንዳንዱ 9/11 ለማከናወን የሚፈልጉትን ማንኛውንም ዓይነት ደግነት እና አገልግሎት ይመርጣሉ። እና ትላልቅ እና ትናንሽ ድርጊቶች, በተቸገሩ ሰዎች እና ማህበረሰቦች ህይወት ላይ እውነተኛ ለውጥ እያመጡ ነው. አንዳንዶች መዋጮ ያደርጋሉ - ልብስ፣ መጽሐፍት፣ የዓይን መነፅር፣ ገንዘብ፣ ደም። አንዳንዶች ትምህርት ቤቶችን እና ፓርኮችን እና የባህር ዳርቻዎችን ለመጠገን ይረዳሉ። አንዳንዶች የእንክብካቤ ፓኬጆችን ወደ ባህር ማዶ ለወታደሮቻችን ይልካሉ ወይም በሾርባ ኩሽና ውስጥ ይሰራሉ። አንዳንዶች ዓይነ ስውራን ያነባሉ ወይም አረጋውያንን ይጎበኛሉ። ዛሬ እሑድ ጡረተኛው የኒውዮርክ ጃይንት ታላቁ ጆርጅ ማርቲን ከ9/ በኋላ ባደረጉት አገልግሎት ምክንያት በህመም ለሚሰቃዩ በሺዎች ለሚቆጠሩ የ Ground Zero የማዳን እና የማገገሚያ ሰራተኞች የጤና እንክብካቤ ገንዘብ ለማሰባሰብ ከኒውዮርክ ወደ ኒው ጀርሲ የገንዘብ ማሰባሰብያ የእግር ጉዞ እየመራ ነው። 11. ሰዎች ለመሳተፍ ምን ማድረግ እንደሚችሉ ምንም ገደብ የለም. ልብ ልንል የሚገባዉ ልክ በዚያ መስከረም ጧት በፍርሃት እንደሞቱት ሁሉ በ9/11 አገልግሎት ላይ የተሰማሩ ሰዎች የተለያዩ የፖለቲካ ምርጫዎችን፣ እድሜን፣ ዘርን፣ ሀይማኖቶችን፣ ኢኮኖሚያዊ ደረጃን እና ጂኦግራፊያዊ አቀማመጥን ያመለክታሉ። ሰሞኑን በአንዳንድ ወግ አጥባቂ ጋዜጠኞች እና ጦማሪያን ከተጠቆመው በተለየ ይህ የ9/11 አገልግሎት ክስተት አዲስ አይደለም እና በእርግጠኝነት የሴፕቴምበር 11ን ትርጉም ለማሳነስ ወይም ለፖለቲካዊ ጥቅም ሲል እንደገና ለመወሰን የታሰበ የሊበራል አጀንዳ አይደለም። በእርግጥም እያንዳንዱን 9/11 ለማክበር በአገልግሎት ላይ የመሳተፍ ሀሳብ የተጀመረው በቡሽ አስተዳደር ጊዜ ሲሆን በሁለቱም በኩል በካፒቶል ሂል ላይ በሰፊው ተደግፏል። በ2004 የዩኤስ ኮንግረስ የሴፕቴምበር 11 ቀን ብሔራዊ የአገልግሎት እና የርህራሄ ቀን መሆን አለበት በማለት የ House Congressional Resolution 473 በሙሉ ድምጽ አጽድቋል። በዚህ አመት በሚያዝያ ወር፣ ፕሬዘዳንት ኦባማ ኤድዋርድ ኤም ኬኔዲ አሜሪካን ሰርቪስ ህግን ተፈራርመዋል፣ መስከረም 11ን እንደ \"ብሄራዊ የአገልግሎት እና መታሰቢያ ቀን\" በመደበኛነት የሚያቋቁመውን የሁለት ወገን ህግ ነው። የ9/11 ማህበረሰብ ይህንን ትልቅ እርምጃ አድንቆታል። እናም ፕሬዝዳንቱ ይህን ህግ ሲፈርሙ ስመለከት ከሰርቪስ ኔሽን ጥምር አባላት ጋር እና በተለያዩ የፖለቲካ መሪዎች፣ አስተማሪዎች፣ ተማሪዎች፣ የአገልግሎት በጎ ፈቃደኞች እና ሌሎች የተከበቡ፣ እኔ ሳስበው ሁለቱንም ብርድ ብርድ ማለት እና ታላቅ ሙቀት ተሰማኝ ሌሎችን በማገልገል የሞተው ወንድሜ። በሴፕቴምበር 11 የተጎዱትን ፣ሰራተኞችን እና በጎ ፍቃደኞችን ለማክበር እንደ መንገድ አገልግሎት መጪው ትውልድ ስለ ጥቃቶቹ ብቻ ሳይሆን በአለም ላይ ያሉ ጥሩ ሰዎች ሀገራችን በከባድ ጉዳት በደረሰባት ጊዜ ምን ምላሽ እንደሰጡ ይማራሉ ። በተጨማሪም ከሴፕቴምበር 11 ቀን አመድ ለመውጣት ውጤታማ እና ትርጉም ያለው መንገድ ሆኖ ያገለግላል ይህም አገራዊ ፈተናዎቻችን እንደቀድሞው ሁሉ አሁን ናቸው። አንዳንድ ሰዎች \"በሴፕቴምበር 11 ላይ በቂ ነው. ወደ ፊት እንቀጥል\" ይላሉ. በእርግጠኝነት, ያንን ተረድቻለሁ. እያስታወስን እንኳን ወደፊት መሄድ አለብን። እናም በዚህ በዓል ላይ ዋናው ነጥብ ይህ ነው። ለተጎዱት ወይም ለጠፉት እንደ ወንድሜ እና አገራችንን ለመታደግ እና ለመጠበቅ በአገልግሎት ለተነሱት ሁሉ በጉጉት በመመልከት እና የተቸገሩትን ሰዎች እና ማህበረሰቦችን በማሻሻል ክብር እንስጥ። በታሪክ ውስጥ ይህ ቅጽበት ስለ ጥቃቶቹ ብቻ አልነበረም። ስለ ርህራሄም ነበር። ይህ ደግሞ የ9/11 ክስተቶች ትምህርት ነው፣ እና ለእኔ የዚህ ብሔራዊ የአገልግሎት እና የመታሰቢያ ቀን ይዘት ነው። በዚህ አስተያየት ውስጥ የተገለጹት አስተያየቶች የጄ ኤስ.ዊኑክ ብቻ ናቸው።\\n\\n### End'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "   \n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "  \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"summary\", and \"text\" fields\n",
    "    preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"summary\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Random seed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m22\u001b[39m\n\u001b[0;32m----> 4\u001b[0m max_length \u001b[38;5;241m=\u001b[39m get_max_length(\u001b[43mmodel\u001b[49m)\n\u001b[1;32m      5\u001b[0m preprocessed_dataset \u001b[38;5;241m=\u001b[39m preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 22\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lora_alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m----> 2\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[43mlora_alpha\u001b[49m,\n\u001b[1;32m      3\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39mlora_dropout,\n\u001b[1;32m      4\u001b[0m     r\u001b[38;5;241m=\u001b[39mlora_r,\n\u001b[1;32m      5\u001b[0m     bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lora_alpha' is not defined"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
