# -*- coding: utf-8 -*-
"""load_garri_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNqhL8WG_zDe8n-g4wllBg8hGqee82rp
"""


from google.colab import drive
drive.mount('/content/drive')

from transformers import LlamaTokenizer

checkpoint = "iocuydi/llama-2-amharic-3784m"
commit_hash = "04fcac974701f1dab0b8e39af9d3ecfce07b3773"

tokenizer = LlamaTokenizer.from_pretrained(checkpoint, revision =commit_hash)



print(tokenizer.encode("ሰላም፣ አንዴት ነሽ?"))
print(tokenizer.tokenize("ሰላም፣ አንዴት ነሽ?"))



tokenizer.vocab_size

import torch

torch.cuda.is_available()

from huggingface_hub import notebook_login
notebook_login()

from peft import PeftModel
from transformers import AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig

# Create a BitsAndBytesConfig object for 8-bit quantization
quant_config = BitsAndBytesConfig(
    load_in_8bit=True
)

# Load the model with the quantization configuration
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quant_config,
    device_map="auto",
)



model.resize_token_embeddings(len(tokenizer))



peftmodel = PeftModel.from_pretrained(model, "iocuydi/llama-2-amharic-3784m",revision =commit_hash)



model.is_quantized

len(model.model.layers)

def chat_with_llama(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    input_ids = input_ids.to('cuda')
    output = model.generate(input_ids, max_length=256, num_beams=4, no_repeat_ngram_size=2)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

while True:
    prompt = input("You: ")
    response = chat_with_llama(prompt)
    print("Llama:", response)